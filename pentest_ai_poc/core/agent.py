import json
from typing import Dict, Generator, Optional

import requests

from core.logger import logger


class AIAgent:
    """
    使用本地 Ollama 模型 qwen3-coder:30b 进行推理的简易封装。
    """

    def __init__(
        self,
        model: str = "qwen3-coder:30b",
        endpoint: str = "http://localhost:11434/api/generate",
        temperature: float = 0.3,
        timeout: int = 300,
    ) -> None:
        self.model = model
        self.endpoint = endpoint
        self.temperature = temperature
        self.timeout = timeout

    def _stream_generate(self, prompt: str) -> Generator[str, None, None]:
        payload: Dict[str, object] = {
            "model": self.model,
            "prompt": prompt,
            "stream": True,
            "options": {"temperature": self.temperature},
        }

        with requests.post(
            self.endpoint, json=payload, stream=True, timeout=self.timeout
        ) as resp:
            resp.raise_for_status()
            for line in resp.iter_lines(decode_unicode=True):
                if not line:
                    continue
                # Ollama 流式返回为一行一个 JSON
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                chunk: Optional[str] = data.get("response")  # type: ignore[assignment]
                if isinstance(chunk, str):
                    yield chunk

    def generate_report(self, prompt: str) -> str:
        """
        调用 Ollama 生成完整报告（内部使用流式接口并拼接）。
        """
        logger.info(f"[agent] Calling Ollama model '{self.model}' to generate report")
        chunks = []
        try:
            for piece in self._stream_generate(prompt):
                chunks.append(piece)
            logger.info("[agent] AI report generated successfully")
        except requests.RequestException as exc:
            logger.error(f"[agent] Ollama API call failed: {exc}")
            return f"[ERROR] 调用 Ollama 失败: {exc}"

        return "".join(chunks).strip()

